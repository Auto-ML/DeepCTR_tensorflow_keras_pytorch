{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"Data/train.csv.gz\")\n",
    "    return df\n",
    "\n",
    "def oneHotEncoder(array_1d):\n",
    "    label = LabelEncoder().fit_transform(array_1d)\n",
    "    label = label.reshape(len(label), 1)\n",
    "    one_hot = OneHotEncoder(sparse=False).fit_transform(label)\n",
    "    return one_hot\n",
    "\n",
    "def minMaxScale(array_2d):\n",
    "    return MinMaxScaler().fit_transform(array_2d)\n",
    "\n",
    "def preprocess(data):\n",
    "    cat_list =[f for f in data.columns]\n",
    "    for c in cat_list:\n",
    "        data[c] = LabelEncoder().fit_transform(list(data[c].values))\n",
    "\n",
    "    return data\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "def eval_matric(y_true, y_prob):\n",
    "    print(sum(y_true)/ len(y_true))\n",
    "    print(sum([i>0.5 for i in y_prob])/ len(y_true))\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "        gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    print(\"gini:\", gini)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.models import DeepFM, xDeepFM, DCN\n",
    "from deepctr.inputs import  SparseFeat, DenseFeat,get_fixlen_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---loading and preprocessing the data---\n"
     ]
    }
   ],
   "source": [
    "print(\"---loading and preprocessing the data---\")\n",
    "data = load_data()\n",
    "data = data.set_index(\"id\")\n",
    "target = data['target']\n",
    "data.drop(['target'], axis=1, inplace=True)\n",
    "#data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse : unique sum  376\n",
      "27 30\n",
      "(595212, 57)\n",
      "(595212, 57)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "from deepctr.models import DeepFM\n",
    "from deepctr.inputs import  SparseFeat, DenseFeat,get_fixlen_feature_names\n",
    "\n",
    "def recognize_feature(data, label_encoder = False):\n",
    "    sparse_features = []\n",
    "    dense_features = []\n",
    "    for f in data.columns:\n",
    "        if data[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(data[f].values))\n",
    "            data[f] = lbl.transform(list(data[f].values))\n",
    "            sparse_features.append(f)\n",
    "        elif f.find('cat') >=0 and f.find('bin') <0:\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(data[f].values))\n",
    "            data[f] = lbl.transform(list(data[f].values))\n",
    "            sparse_features.append(f)\n",
    "        elif data[f].dtype not in ['float16','float32','float64']:\n",
    "            if(len(data[f].unique()) < 100 and f.find('bin') <0):\n",
    "                lbl = LabelEncoder()\n",
    "                lbl.fit(list(data[f].values))\n",
    "                data[f] = lbl.transform(list(data[f].values))\n",
    "                sparse_features.append(f)\n",
    "    print(\"sparse : unique sum \", sum([len(data[f].unique()) for f in sparse_features]))\n",
    "        \n",
    "    dense_features = list(set(data.columns.tolist()) - set(sparse_features))\n",
    "    return data, sparse_features, dense_features\n",
    "\n",
    "def hash_encoding(data, sparse_features):\n",
    "    return ;\n",
    "def one_hot_for_sparse(data, sparse_features):\n",
    "    for f in sparse_features:\n",
    "        one_hot = pd.get_dummies(data[f], prefix =f, dummy_na = True)\n",
    "        data.drop(f , axis = 1, inplace=True)\n",
    "        data = data.join(one_hot)\n",
    "    return data\n",
    "def scalar_for_dense(data, dense_features):\n",
    "    for f in dense_features:\n",
    "        scaler = MinMaxScaler()\n",
    "        data[f] = scaler.fit_transform(data[f].values.reshape(-1,1))\n",
    "    return data\n",
    "    \n",
    "\n",
    "data, sparse_features, dense_features = recognize_feature(data)\n",
    "print(len(dense_features),len(sparse_features))\n",
    "\n",
    "sparse_label_dict = dict()\n",
    "for f in sparse_features:\n",
    "    sparse_label_dict[f] = data[f].max()\n",
    "\n",
    "print(data.shape)\n",
    "#data = one_hot_for_sparse(data, sparse_features)\n",
    "#data = scalar_for_dense(data, dense_features)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sparse_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense, concatenate,Dropout,BatchNormalization,Activation,Flatten,Add\n",
    "from keras.layers import RepeatVector, merge, Subtract, Lambda, Multiply, Embedding, Concatenate, Reshape\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class Added_Weights(Layer):\n",
    "    def __init__(self, use_bias = False, **kwargs):\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        super(Added_Weights, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[1], input_shape[2]),\n",
    "                                      initializer='uniform',  # TODO: Choose your initializer\n",
    "                                      trainable=True)\n",
    "        \n",
    "        if(self.use_bias):\n",
    "            self.bias = self.add_weight(name='bias',\n",
    "                                        shape=(1, input_shape[2]),\n",
    "                                        initializer='uniform',  # TODO: Choose your initializer\n",
    "                                        trainable=True)\n",
    "        else:\n",
    "            self.bias = self.add_weight(name='bias',\n",
    "                                        shape=(1, input_shape[2]),\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=False)\n",
    "        \n",
    "        super(Added_Weights, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        # Implicit broadcasting occurs here.\n",
    "        # Shape x: (BATCH_SIZE, N, M)\n",
    "        # Shape kernel: (N, M)\n",
    "        # Shape output: (BATCH_SIZE, N, M)\n",
    "        #if self.use_bias:\n",
    "        #if self.use_bias:\n",
    "        return x * self.kernel + self.bias\n",
    "        \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "class DeepFM():\n",
    "    def __init__(self, sparse_features, dense_features, sparse_label_dict, hidden_layer, embed_dim):\n",
    "        self.sparse_features = sparse_features\n",
    "        self.dense_features = dense_features\n",
    "        self.sparse_label_dict = sparse_label_dict\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def fit(self, train, test, y_train, y_test):\n",
    "        cat_input = []\n",
    "        cat_output = []        \n",
    "        for col in self.sparse_features:\n",
    "            input = Input(shape= (1,))\n",
    "            cat_input.append(input)\n",
    "            emb = Embedding(sparse_label_dict[col], self.embed_dim, input_length =1 ,trainable = True)(input)\n",
    "            cat_output.append(emb)\n",
    "         \n",
    "        cat_output = Concatenate(axis=1)(cat_output)\n",
    "        \n",
    "        first_order = Added_Weights(use_bias = True)(cat_output)\n",
    "        first_order = Flatten()(first_order)\n",
    "        \n",
    "        # 需要使用lambda 层封装Backend 的函数操作\n",
    "        first_order = Lambda(lambda x: K.sum(x, axis =1, keepdims=True))(first_order)\n",
    "        \n",
    "        # cat_output shape : s *k, keras 需要把这个list 进行concat 为一个tensor\n",
    "        # 然后fatten 为一个weight，然后在sum，或者是直接sum, w * x ,w 是tf.variable\n",
    "        \n",
    "        # second order for sparse features with fixed dim\n",
    "        # vx * vx - vx, vx shape: (1, k)\n",
    "        vx = Added_Weights()(cat_output)\n",
    "        sum_square = Lambda(lambda x: K.sum(x, axis =1))(vx)\n",
    "        sum_square = Multiply()([sum_square, sum_square])\n",
    "        square_sum = Multiply()([vx, vx])\n",
    "        square_sum = Lambda(lambda x: K.sum(x, axis =1))(square_sum)\n",
    "        second_order = Subtract()([sum_square, square_sum])\n",
    "        second_order = Lambda(lambda x: K.sum(x/2, axis =1, keepdims=True))(second_order)\n",
    "        print(second_order.shape)\n",
    "        '''\n",
    "        dense_input = []\n",
    "        for col in self.dense_features:\n",
    "            input = Input(shape = (1, ))\n",
    "            dense_input.append(input)\n",
    "        dense_input = Concatenate(axis=1)(dense_input)\n",
    "        '''\n",
    "        dense_input = Input(shape = (len(self.dense_features), ))\n",
    "        \n",
    "        dnn_input = Concatenate(axis=1)([Flatten()(cat_output), dense_input])\n",
    "        #dnn_input = dense_input \n",
    "        dnn_output = dnn_input \n",
    "        for layer in self.hidden_layer:\n",
    "            dnn_output  = BatchNormalization()(dnn_output)\n",
    "            dnn_output  = Dense(layer, activation='relu')(dnn_output)\n",
    "            dnn_output  = Dropout(0.2)(dnn_output)\n",
    "        dnn_output = Dense(1, activation='linear')(dnn_output)\n",
    "        \n",
    "        #output  = Concatenate(axis=1)([first_order, second_order, dnn_output])\n",
    "        output  = Add()([first_order, second_order, dnn_output])\n",
    "        output = Dense(1, activation='sigmoid')(output)\n",
    "        model = Model(inputs = cat_input + [dense_input], outputs=output)\n",
    "        print(\"---starting the training---\")\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        #print(model.summary())\n",
    "        model.fit([train[f] for f in self.sparse_features] + [train[self.dense_features]], y_train, nb_epoch=50, batch_size=1000)\n",
    "        loss, accuracy = model.evaluate([test[f] for f in self.sparse_features] +  [test[self.dense_features]], y_test)\n",
    "        print('\\n', 'test accuracy:', accuracy)\n",
    "        y_pred = model.predict([test[f] for f in self.sparse_features] +  [test[self.dense_features]])\n",
    "        print(sum(y_test))\n",
    "        print(len(y_test))\n",
    "        print(\"auc is \", roc_auc_score(y_test, y_pred))\n",
    "        eval_matric(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n",
      "---starting the training---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "398792/398792 [==============================] - 22s 55us/step - loss: 0.1629 - acc: 0.9634\n",
      "Epoch 2/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.1525 - acc: 0.9637\n",
      "Epoch 3/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.1515 - acc: 0.9637\n",
      "Epoch 4/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.1509 - acc: 0.9637\n",
      "Epoch 5/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.1497 - acc: 0.9637\n",
      "Epoch 6/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.1483 - acc: 0.9637\n",
      "Epoch 7/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.1462 - acc: 0.9637\n",
      "Epoch 8/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.1423 - acc: 0.9636\n",
      "Epoch 9/50\n",
      "398792/398792 [==============================] - 13s 34us/step - loss: 0.1357 - acc: 0.9636\n",
      "Epoch 10/50\n",
      "398792/398792 [==============================] - 13s 34us/step - loss: 0.1258 - acc: 0.9639\n",
      "Epoch 11/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.1116 - acc: 0.9650\n",
      "Epoch 12/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0962 - acc: 0.9678\n",
      "Epoch 13/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0811 - acc: 0.9717\n",
      "Epoch 14/50\n",
      "398792/398792 [==============================] - 13s 34us/step - loss: 0.0686 - acc: 0.9759\n",
      "Epoch 15/50\n",
      "398792/398792 [==============================] - 13s 33us/step - loss: 0.0586 - acc: 0.9792\n",
      "Epoch 16/50\n",
      "398792/398792 [==============================] - 13s 33us/step - loss: 0.0501 - acc: 0.9822\n",
      "Epoch 17/50\n",
      "398792/398792 [==============================] - 13s 33us/step - loss: 0.0445 - acc: 0.9843\n",
      "Epoch 18/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0388 - acc: 0.9861\n",
      "Epoch 19/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0363 - acc: 0.9872\n",
      "Epoch 20/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0325 - acc: 0.9883\n",
      "Epoch 21/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0303 - acc: 0.9895\n",
      "Epoch 22/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0292 - acc: 0.9897\n",
      "Epoch 23/50\n",
      "398792/398792 [==============================] - 13s 34us/step - loss: 0.0271 - acc: 0.9904\n",
      "Epoch 24/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0251 - acc: 0.9913\n",
      "Epoch 25/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0243 - acc: 0.9914\n",
      "Epoch 26/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0242 - acc: 0.9916\n",
      "Epoch 27/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0219 - acc: 0.9924\n",
      "Epoch 28/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0220 - acc: 0.9925\n",
      "Epoch 29/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0214 - acc: 0.9926\n",
      "Epoch 30/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0201 - acc: 0.9931\n",
      "Epoch 31/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0200 - acc: 0.9932\n",
      "Epoch 32/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0188 - acc: 0.9935\n",
      "Epoch 33/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0190 - acc: 0.9935\n",
      "Epoch 34/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0184 - acc: 0.9938\n",
      "Epoch 35/50\n",
      "398792/398792 [==============================] - 13s 34us/step - loss: 0.0180 - acc: 0.9939\n",
      "Epoch 36/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0178 - acc: 0.9937\n",
      "Epoch 37/50\n",
      "398792/398792 [==============================] - 13s 34us/step - loss: 0.0173 - acc: 0.9941\n",
      "Epoch 38/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0164 - acc: 0.9944\n",
      "Epoch 39/50\n",
      "398792/398792 [==============================] - 13s 33us/step - loss: 0.0164 - acc: 0.9943\n",
      "Epoch 40/50\n",
      "398792/398792 [==============================] - 13s 33us/step - loss: 0.0161 - acc: 0.9945\n",
      "Epoch 41/50\n",
      "398792/398792 [==============================] - 13s 34us/step - loss: 0.0158 - acc: 0.9947\n",
      "Epoch 42/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0156 - acc: 0.9946\n",
      "Epoch 43/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0149 - acc: 0.9948\n",
      "Epoch 44/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0150 - acc: 0.9947\n",
      "Epoch 45/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0143 - acc: 0.9952\n",
      "Epoch 46/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0147 - acc: 0.9950\n",
      "Epoch 47/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0137 - acc: 0.9955\n",
      "Epoch 48/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0140 - acc: 0.9953\n",
      "Epoch 49/50\n",
      "398792/398792 [==============================] - 13s 34us/step - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 50/50\n",
      "398792/398792 [==============================] - 14s 34us/step - loss: 0.0134 - acc: 0.9954\n",
      "196420/196420 [==============================] - 62s 318us/step\n",
      "\n",
      " test accuracy: 0.9619488850422564\n",
      "7202\n",
      "196420\n",
      "auc is  0.5707878363803417\n",
      "0.03666632725791671\n",
      "[0.00171062]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini: [nan]\n"
     ]
    }
   ],
   "source": [
    "model = DeepFM(sparse_features, dense_features, sparse_label_dict, [2048, 1024, 100, 50] , 4)\n",
    "model.fit(train, test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense, concatenate,Dropout,BatchNormalization,Activation\n",
    "from keras.layers import RepeatVector, merge, Subtract, Lambda, Multiply\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import Layer\n",
    "            \n",
    "def models(train, y_train, test, y_test):\n",
    "    # wide\n",
    "    wide = Sequential()\n",
    "    wide = Input(shape=(train.shape[1],))\n",
    "\n",
    "    # deep\n",
    "    deep_data = Input(shape=(train.shape[1],))\n",
    "    deep = Dense(1000, activation='relu')(deep_data)\n",
    "    #deep = BatchNormalization()(deep)\n",
    "    deep = Dropout(0.2)(deep)\n",
    "    \n",
    "    deep = Dense(100, activation='relu')(deep)\n",
    "    #deep = BatchNormalization()(deep)\n",
    "    deep = Dropout(0.2)(deep)\n",
    "\n",
    "    deep = Dense(50, activation='relu')(deep)\n",
    "    #deep = BatchNormalization()(deep)\n",
    "    deep = Dropout(0.2)(deep)\n",
    "    # wide & deep \n",
    "    #wide_deep = concatenate([wide, deep])\n",
    "    wide_deep = deep\n",
    "    #wide_deep = deep\n",
    "    wide_deep = Dense(1, activation='sigmoid')(wide_deep)\n",
    "    model = Model(inputs=[wide, deep_data], outputs=wide_deep)\n",
    "    \n",
    "    sgd = optimizers.SGD(lr=0.1)\n",
    "    adam = optimizers.Adam(lr=0.000)\n",
    "    \n",
    "    print(\"---starting the training---\")\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.fit([train, train], y_train, nb_epoch=20, batch_size=1000)\n",
    "\n",
    "    loss, accuracy = model.evaluate([test, test], y_test)\n",
    "    print('\\n', 'test accuracy:', accuracy)\n",
    "    y_pred = model.predict([test,test])\n",
    "    print(sum(y_test))\n",
    "    print(len(y_test))\n",
    "    print(\"auc is \", roc_auc_score(y_test, y_pred))\n",
    "    eval_matric(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = one_hot_for_sparse(data, sparse_features)\n",
    "#d = scalar_for_dense(data, dense_features)\n",
    "train, test, y_train, y_test = train_test_split(d, target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---starting the training---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "398792/398792 [==============================] - 19s 48us/step - loss: 0.6049 - acc: 0.9624\n",
      "Epoch 2/20\n",
      "398792/398792 [==============================] - 18s 46us/step - loss: 0.5858 - acc: 0.9637\n",
      "Epoch 3/20\n",
      "398792/398792 [==============================] - 19s 47us/step - loss: 0.5857 - acc: 0.9637\n",
      "Epoch 4/20\n",
      "398792/398792 [==============================] - 18s 46us/step - loss: 0.5858 - acc: 0.9637\n",
      "Epoch 5/20\n",
      "398792/398792 [==============================] - 19s 47us/step - loss: 0.5858 - acc: 0.9637\n",
      "Epoch 6/20\n",
      "398792/398792 [==============================] - 19s 46us/step - loss: 0.5857 - acc: 0.9637\n",
      "Epoch 7/20\n",
      "398792/398792 [==============================] - 19s 47us/step - loss: 0.5857 - acc: 0.9637\n",
      "Epoch 8/20\n",
      "398792/398792 [==============================] - 19s 47us/step - loss: 0.5858 - acc: 0.9637\n",
      "Epoch 9/20\n",
      "398792/398792 [==============================] - 19s 46us/step - loss: 0.5858 - acc: 0.9637\n",
      "Epoch 10/20\n",
      "398792/398792 [==============================] - 18s 46us/step - loss: 0.5858 - acc: 0.9637\n",
      "Epoch 11/20\n",
      "398792/398792 [==============================] - 19s 47us/step - loss: 0.5858 - acc: 0.9637\n",
      "Epoch 12/20\n",
      "398792/398792 [==============================] - 19s 47us/step - loss: 0.5858 - acc: 0.9637\n",
      "Epoch 13/20\n",
      "398792/398792 [==============================] - 19s 47us/step - loss: 0.5857 - acc: 0.9637\n",
      "Epoch 14/20\n",
      "398792/398792 [==============================] - 18s 46us/step - loss: 0.5857 - acc: 0.9637\n",
      "Epoch 15/20\n",
      "398792/398792 [==============================] - 19s 47us/step - loss: 0.5857 - acc: 0.9637\n",
      "Epoch 16/20\n",
      "263000/398792 [==================>...........] - ETA: 6s - loss: 0.5872 - acc: 0.9636"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-931fa1963997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-810712f77ea9>\u001b[0m in \u001b[0;36mmodels\u001b[0;34m(train, y_train, test, y_test)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 185\u001b[0;31m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models(train, y_train, test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
